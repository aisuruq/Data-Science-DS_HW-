{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.3.7)\n",
      "Requirement already satisfied: model-signing in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kagglehub) (0.2.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kagglehub) (24.2)\n",
      "Requirement already satisfied: requests in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: cryptography in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from model-signing->kagglehub) (44.0.1)\n",
      "Requirement already satisfied: in-toto-attestation in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from model-signing->kagglehub) (0.9.3)\n",
      "Requirement already satisfied: sigstore in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from model-signing->kagglehub) (3.6.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from model-signing->kagglehub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->kagglehub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->kagglehub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->kagglehub) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cryptography->model-signing->kagglehub) (1.17.1)\n",
      "Requirement already satisfied: protobuf in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from in-toto-attestation->model-signing->kagglehub) (5.29.3)\n",
      "Requirement already satisfied: id>=1.1.0 in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sigstore->model-signing->kagglehub) (1.5.0)\n",
      "Requirement already satisfied: pyasn1~=0.6 in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sigstore->model-signing->kagglehub) (0.6.1)\n",
      "Requirement already satisfied: pydantic<3,>=2 in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sigstore->model-signing->kagglehub) (2.10.6)\n",
      "Requirement already satisfied: pyjwt>=2.1 in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sigstore->model-signing->kagglehub) (2.10.1)\n",
      "Requirement already satisfied: pyOpenSSL>=23.0.0 in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sigstore->model-signing->kagglehub) (25.0.0)\n",
      "Requirement already satisfied: rich~=13.0 in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sigstore->model-signing->kagglehub) (13.9.4)\n",
      "Requirement already satisfied: rfc8785~=0.1.2 in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sigstore->model-signing->kagglehub) (0.1.4)\n",
      "Requirement already satisfied: rfc3161-client~=0.1.2 in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sigstore->model-signing->kagglehub) (0.1.2)\n",
      "Requirement already satisfied: sigstore-protobuf-specs==0.3.2 in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sigstore->model-signing->kagglehub) (0.3.2)\n",
      "Requirement already satisfied: sigstore-rekor-types==0.0.18 in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sigstore->model-signing->kagglehub) (0.0.18)\n",
      "Requirement already satisfied: tuf~=5.0 in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sigstore->model-signing->kagglehub) (5.1.0)\n",
      "Requirement already satisfied: platformdirs~=4.2 in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sigstore->model-signing->kagglehub) (4.3.6)\n",
      "Requirement already satisfied: betterproto==2.0.0b6 in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sigstore-protobuf-specs==0.3.2->sigstore->model-signing->kagglehub) (2.0.0b6)\n",
      "Requirement already satisfied: grpclib<0.5.0,>=0.4.1 in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from betterproto==2.0.0b6->sigstore-protobuf-specs==0.3.2->sigstore->model-signing->kagglehub) (0.4.7)\n",
      "Requirement already satisfied: python-dateutil<3.0,>=2.8 in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from betterproto==2.0.0b6->sigstore-protobuf-specs==0.3.2->sigstore->model-signing->kagglehub) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cffi>=1.12->cryptography->model-signing->kagglehub) (2.22)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=2->sigstore->model-signing->kagglehub) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=2->sigstore->model-signing->kagglehub) (2.27.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rich~=13.0->sigstore->model-signing->kagglehub) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rich~=13.0->sigstore->model-signing->kagglehub) (2.19.1)\n",
      "Requirement already satisfied: securesystemslib~=1.0 in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tuf~=5.0->sigstore->model-signing->kagglehub) (1.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from markdown-it-py>=2.2.0->rich~=13.0->sigstore->model-signing->kagglehub) (0.1.2)\n",
      "Requirement already satisfied: email-validator>=2.0.0 in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic[email]<3,>=2->sigstore-rekor-types==0.0.18->sigstore->model-signing->kagglehub) (2.2.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from email-validator>=2.0.0->pydantic[email]<3,>=2->sigstore-rekor-types==0.0.18->sigstore->model-signing->kagglehub) (2.7.0)\n",
      "Requirement already satisfied: h2<5,>=3.1.0 in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from grpclib<0.5.0,>=0.4.1->betterproto==2.0.0b6->sigstore-protobuf-specs==0.3.2->sigstore->model-signing->kagglehub) (4.2.0)\n",
      "Requirement already satisfied: multidict in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from grpclib<0.5.0,>=0.4.1->betterproto==2.0.0b6->sigstore-protobuf-specs==0.3.2->sigstore->model-signing->kagglehub) (6.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil<3.0,>=2.8->betterproto==2.0.0b6->sigstore-protobuf-specs==0.3.2->sigstore->model-signing->kagglehub) (1.17.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from h2<5,>=3.1.0->grpclib<0.5.0,>=0.4.1->betterproto==2.0.0b6->sigstore-protobuf-specs==0.3.2->sigstore->model-signing->kagglehub) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in c:\\users\\2814\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from h2<5,>=3.1.0->grpclib<0.5.0,>=0.4.1->betterproto==2.0.0b6->sigstore-protobuf-specs==0.3.2->sigstore->model-signing->kagglehub) (4.1.0)\n",
      "template load succesfull!\n"
     ]
    }
   ],
   "source": [
    "%run ./../template.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.7), please consider upgrading to the latest version (0.3.8).\n",
      "Path to dataset files: C:\\Users\\2814\\.cache\\kagglehub\\datasets\\rohanrao\\air-quality-data-in-india\\versions\\12\n"
     ]
    }
   ],
   "source": [
    "path = kagglehub.dataset_download(\"rohanrao/air-quality-data-in-india\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(config['ml']['data_folder'])\n",
    "shutil.move(path, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(data_path / \"12/city_day.csv\")\n",
    "val_df = pd.read_csv(data_path / \"12/city_day.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Date</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>NO</th>\n",
       "      <th>NO2</th>\n",
       "      <th>NOx</th>\n",
       "      <th>NH3</th>\n",
       "      <th>CO</th>\n",
       "      <th>SO2</th>\n",
       "      <th>O3</th>\n",
       "      <th>Benzene</th>\n",
       "      <th>Toluene</th>\n",
       "      <th>Xylene</th>\n",
       "      <th>AQI</th>\n",
       "      <th>AQI_Bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ahmedabad</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>18.22</td>\n",
       "      <td>17.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>27.64</td>\n",
       "      <td>133.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ahmedabad</td>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.97</td>\n",
       "      <td>15.69</td>\n",
       "      <td>16.46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.97</td>\n",
       "      <td>24.55</td>\n",
       "      <td>34.06</td>\n",
       "      <td>3.68</td>\n",
       "      <td>5.50</td>\n",
       "      <td>3.77</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ahmedabad</td>\n",
       "      <td>2015-01-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.40</td>\n",
       "      <td>19.30</td>\n",
       "      <td>29.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.40</td>\n",
       "      <td>29.07</td>\n",
       "      <td>30.70</td>\n",
       "      <td>6.80</td>\n",
       "      <td>16.40</td>\n",
       "      <td>2.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ahmedabad</td>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.70</td>\n",
       "      <td>18.48</td>\n",
       "      <td>17.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.70</td>\n",
       "      <td>18.59</td>\n",
       "      <td>36.08</td>\n",
       "      <td>4.43</td>\n",
       "      <td>10.14</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ahmedabad</td>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.10</td>\n",
       "      <td>21.42</td>\n",
       "      <td>37.76</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.10</td>\n",
       "      <td>39.33</td>\n",
       "      <td>39.31</td>\n",
       "      <td>7.01</td>\n",
       "      <td>18.89</td>\n",
       "      <td>2.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        City        Date  PM2.5  PM10     NO    NO2    NOx  NH3     CO    SO2  \\\n",
       "0  Ahmedabad  2015-01-01    NaN   NaN   0.92  18.22  17.15  NaN   0.92  27.64   \n",
       "1  Ahmedabad  2015-01-02    NaN   NaN   0.97  15.69  16.46  NaN   0.97  24.55   \n",
       "2  Ahmedabad  2015-01-03    NaN   NaN  17.40  19.30  29.70  NaN  17.40  29.07   \n",
       "3  Ahmedabad  2015-01-04    NaN   NaN   1.70  18.48  17.97  NaN   1.70  18.59   \n",
       "4  Ahmedabad  2015-01-05    NaN   NaN  22.10  21.42  37.76  NaN  22.10  39.33   \n",
       "\n",
       "       O3  Benzene  Toluene  Xylene  AQI AQI_Bucket  \n",
       "0  133.36     0.00     0.02    0.00  NaN        NaN  \n",
       "1   34.06     3.68     5.50    3.77  NaN        NaN  \n",
       "2   30.70     6.80    16.40    2.25  NaN        NaN  \n",
       "3   36.08     4.43    10.14    1.00  NaN        NaN  \n",
       "4   39.31     7.01    18.89    2.78  NaN        NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29531, 16)\n"
     ]
    }
   ],
   "source": [
    "print(val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пропущенные поля\n",
    "print(train_df.isnull().sum())\n",
    "# Убираем колонки \n",
    "train_df.drop(columns=[\"City\", \"Date\"], inplace=True)\n",
    "# Заполнение NaN (например, медианой или средним) - надо ли нам это?\n",
    "# train_df.head().fillna(train_df.head().median(), inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Date  PM2.5     CO      O3\n",
      "0     2015-01-01  58.37   0.92  133.36\n",
      "1     2015-01-02  58.37   0.97   34.06\n",
      "2     2015-01-03  58.37  17.40   30.70\n",
      "3     2015-01-04  58.37   1.70   36.08\n",
      "4     2015-01-05  58.37  22.10   39.31\n",
      "...          ...    ...    ...     ...\n",
      "2004  2020-06-27  62.12   0.49   68.05\n",
      "2005  2020-06-28  31.57   0.52   26.34\n",
      "2006  2020-06-29  29.75   0.67   34.99\n",
      "2007  2020-06-30  40.02   0.73   41.64\n",
      "2008  2020-07-01  37.63   0.28    9.69\n",
      "\n",
      "[2009 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Здесь мы фильтруем данные для города \"Ahmedabad\" и выбираем столбцы \"Date\", \"PM2.5\", \"CO\", \"O3\"\n",
    "city_data = train_df[train_df[\"City\"] == \"Ahmedabad\"][[\"Date\", \"PM2.5\", \"CO\", \"O3\"]]\n",
    "\n",
    "# Теперь заполним пропуски (NaN) в выбранных столбцах медианой этих столбцов\n",
    "city_data[[\"PM2.5\", \"CO\", \"O3\"]] = city_data[[\"PM2.5\", \"CO\", \"O3\"]].fillna(city_data[[\"PM2.5\", \"CO\", \"O3\"]].median())\n",
    "print(city_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TimeSeriesTransformerForPrediction, TimeSeriesTransformerConfig\n",
    "model_name = \"kleopatra102/air_pollution\"\n",
    "config = TimeSeriesTransformerConfig.from_pretrained(model_name)\n",
    "model = TimeSeriesTransformerForPrediction.from_pretrained(model_name)\n",
    "# print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10, 270)\n",
      "(100, 10, 270)\n",
      "(100, 10)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (270) must match the size of tensor b (10) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Переводим модель в режим оценки\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 19\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_time_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_observed_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstatic_categorical_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatic_categorical_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstatic_real_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatic_real_features\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredictions\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\2814\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\2814\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\2814\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\time_series_transformer\\modeling_time_series_transformer.py:1557\u001b[0m, in \u001b[0;36mTimeSeriesTransformerForPrediction.forward\u001b[1;34m(self, past_values, past_time_features, past_observed_mask, static_categorical_features, static_real_features, future_values, future_time_features, future_observed_mask, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, output_hidden_states, output_attentions, use_cache, return_dict)\u001b[0m\n\u001b[0;32m   1554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m future_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1555\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1557\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_time_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_time_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_observed_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_observed_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_categorical_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatic_categorical_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_real_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatic_real_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfuture_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuture_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfuture_time_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuture_time_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1572\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1573\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1575\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1577\u001b[0m prediction_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1578\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\2814\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\2814\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\2814\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\time_series_transformer\\modeling_time_series_transformer.py:1378\u001b[0m, in \u001b[0;36mTimeSeriesTransformerModel.forward\u001b[1;34m(self, past_values, past_time_features, past_observed_mask, static_categorical_features, static_real_features, future_values, future_time_features, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, output_hidden_states, output_attentions, use_cache, return_dict)\u001b[0m\n\u001b[0;32m   1375\u001b[0m use_cache \u001b[38;5;241m=\u001b[39m use_cache \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[0;32m   1376\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1378\u001b[0m transformer_inputs, loc, scale, static_feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_network_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_time_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_time_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_observed_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_observed_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_categorical_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatic_categorical_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_real_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatic_real_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfuture_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuture_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfuture_time_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuture_time_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1386\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1389\u001b[0m     enc_input \u001b[38;5;241m=\u001b[39m transformer_inputs[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mcontext_length, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\2814\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\time_series_transformer\\modeling_time_series_transformer.py:1269\u001b[0m, in \u001b[0;36mTimeSeriesTransformerModel.create_network_inputs\u001b[1;34m(self, past_values, past_time_features, static_categorical_features, static_real_features, past_observed_mask, future_values, future_time_features)\u001b[0m\n\u001b[0;32m   1267\u001b[0m context \u001b[38;5;241m=\u001b[39m past_values[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mcontext_length :]\n\u001b[0;32m   1268\u001b[0m observed_context \u001b[38;5;241m=\u001b[39m past_observed_mask[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mcontext_length :]\n\u001b[1;32m-> 1269\u001b[0m _, loc, scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobserved_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1271\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1272\u001b[0m     (torch\u001b[38;5;241m.\u001b[39mcat((past_values, future_values), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m loc) \u001b[38;5;241m/\u001b[39m scale\n\u001b[0;32m   1273\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m future_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1274\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (past_values \u001b[38;5;241m-\u001b[39m loc) \u001b[38;5;241m/\u001b[39m scale\n\u001b[0;32m   1275\u001b[0m )\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;66;03m# static features\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\2814\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\2814\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\2814\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\time_series_transformer\\modeling_time_series_transformer.py:145\u001b[0m, in \u001b[0;36mTimeSeriesMeanScaler.forward\u001b[1;34m(self, data, observed_indicator)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m, data: torch\u001b[38;5;241m.\u001b[39mTensor, observed_indicator: torch\u001b[38;5;241m.\u001b[39mTensor\n\u001b[0;32m    133\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    134\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m    Parameters:\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m        data (`torch.Tensor` of shape `(batch_size, sequence_length, num_input_channels)`):\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m            `(batch_size, 1, num_input_channels)`)\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m     ts_sum \u001b[38;5;241m=\u001b[39m (\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mobserved_indicator\u001b[49m)\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    146\u001b[0m     num_observed \u001b[38;5;241m=\u001b[39m observed_indicator\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    148\u001b[0m     scale \u001b[38;5;241m=\u001b[39m ts_sum \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(num_observed, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (270) must match the size of tensor b (10) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "\n",
    "X_test = np.random.randn(100, 10, 270) \n",
    "past_time_features = np.random.randn(100, 10, 270) \n",
    "past_observed_mask = np.ones((100, 10)) \n",
    "static_categorical_features = torch.zeros((100, 5))  \n",
    "static_real_features = torch.zeros((100, 3)) \n",
    "\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "past_time_features = torch.tensor(past_time_features, dtype=torch.float32)\n",
    "past_observed_mask = torch.tensor(past_observed_mask, dtype=torch.float32)\n",
    "\n",
    "\n",
    "model.eval() \n",
    "with torch.no_grad():\n",
    "    predictions = model(\n",
    "        X_test, \n",
    "        past_time_features, \n",
    "        past_observed_mask,\n",
    "        static_categorical_features=static_categorical_features, \n",
    "        static_real_features=static_real_features\n",
    "    )\n",
    "print(f\"Shape of predictions: {predictions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method forward in module transformers.models.time_series_transformer.modeling_time_series_transformer:\n",
      "\n",
      "forward(\n",
      "    past_values: torch.Tensor,\n",
      "    past_time_features: torch.Tensor,\n",
      "    past_observed_mask: torch.Tensor,\n",
      "    static_categorical_features: Optional[torch.Tensor] = None,\n",
      "    static_real_features: Optional[torch.Tensor] = None,\n",
      "    future_values: Optional[torch.Tensor] = None,\n",
      "    future_time_features: Optional[torch.Tensor] = None,\n",
      "    future_observed_mask: Optional[torch.Tensor] = None,\n",
      "    decoder_attention_mask: Optional[torch.LongTensor] = None,\n",
      "    head_mask: Optional[torch.Tensor] = None,\n",
      "    decoder_head_mask: Optional[torch.Tensor] = None,\n",
      "    cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
      "    encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n",
      "    past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
      "    output_hidden_states: Optional[bool] = None,\n",
      "    output_attentions: Optional[bool] = None,\n",
      "    use_cache: Optional[bool] = None,\n",
      "    return_dict: Optional[bool] = None\n",
      ") -> Union[transformers.modeling_outputs.Seq2SeqTSModelOutput, Tuple] method of transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesTransformerForPrediction instance\n",
      "    The [`TimeSeriesTransformerForPrediction`] forward method, overrides the `__call__` special method.\n",
      "\n",
      "        <Tip>\n",
      "\n",
      "        Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\n",
      "        instance afterwards instead of this since the former takes care of running the pre and post processing steps while\n",
      "        the latter silently ignores them.\n",
      "\n",
      "        </Tip>\n",
      "\n",
      "        Args:\n",
      "            past_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, input_size)`):\n",
      "                Past values of the time series, that serve as context in order to predict the future. The sequence size of\n",
      "                this tensor must be larger than the `context_length` of the model, since the model will use the larger size\n",
      "                to construct lag features, i.e. additional values from the past which are added in order to serve as \"extra\n",
      "                context\".\n",
      "\n",
      "                The `sequence_length` here is equal to `config.context_length` + `max(config.lags_sequence)`, which if no\n",
      "                `lags_sequence` is configured, is equal to `config.context_length` + 7 (as by default, the largest\n",
      "                look-back index in `config.lags_sequence` is 7). The property `_past_length` returns the actual length of\n",
      "                the past.\n",
      "\n",
      "                The `past_values` is what the Transformer encoder gets as input (with optional additional features, such as\n",
      "                `static_categorical_features`, `static_real_features`, `past_time_features` and lags).\n",
      "\n",
      "                Optionally, missing values need to be replaced with zeros and indicated via the `past_observed_mask`.\n",
      "\n",
      "                For multivariate time series, the `input_size` > 1 dimension is required and corresponds to the number of\n",
      "                variates in the time series per time step.\n",
      "            past_time_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_features)`):\n",
      "                Required time features, which the model internally will add to `past_values`. These could be things like\n",
      "                \"month of year\", \"day of the month\", etc. encoded as vectors (for instance as Fourier features). These\n",
      "                could also be so-called \"age\" features, which basically help the model know \"at which point in life\" a\n",
      "                time-series is. Age features have small values for distant past time steps and increase monotonically the\n",
      "                more we approach the current time step. Holiday features are also a good example of time features.\n",
      "\n",
      "                These features serve as the \"positional encodings\" of the inputs. So contrary to a model like BERT, where\n",
      "                the position encodings are learned from scratch internally as parameters of the model, the Time Series\n",
      "                Transformer requires to provide additional time features. The Time Series Transformer only learns\n",
      "                additional embeddings for `static_categorical_features`.\n",
      "\n",
      "                Additional dynamic real covariates can be concatenated to this tensor, with the caveat that these features\n",
      "                must but known at prediction time.\n",
      "\n",
      "                The `num_features` here is equal to `config.`num_time_features` + `config.num_dynamic_real_features`.\n",
      "            past_observed_mask (`torch.BoolTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, input_size)`, *optional*):\n",
      "                Boolean mask to indicate which `past_values` were observed and which were missing. Mask values selected in\n",
      "                `[0, 1]`:\n",
      "\n",
      "                - 1 for values that are **observed**,\n",
      "                - 0 for values that are **missing** (i.e. NaNs that were replaced by zeros).\n",
      "\n",
      "            static_categorical_features (`torch.LongTensor` of shape `(batch_size, number of static categorical features)`, *optional*):\n",
      "                Optional static categorical features for which the model will learn an embedding, which it will add to the\n",
      "                values of the time series.\n",
      "\n",
      "                Static categorical features are features which have the same value for all time steps (static over time).\n",
      "\n",
      "                A typical example of a static categorical feature is a time series ID.\n",
      "            static_real_features (`torch.FloatTensor` of shape `(batch_size, number of static real features)`, *optional*):\n",
      "                Optional static real features which the model will add to the values of the time series.\n",
      "\n",
      "                Static real features are features which have the same value for all time steps (static over time).\n",
      "\n",
      "                A typical example of a static real feature is promotion information.\n",
      "            future_values (`torch.FloatTensor` of shape `(batch_size, prediction_length)` or `(batch_size, prediction_length, input_size)`, *optional*):\n",
      "                Future values of the time series, that serve as labels for the model. The `future_values` is what the\n",
      "                Transformer needs during training to learn to output, given the `past_values`.\n",
      "\n",
      "                The sequence length here is equal to `prediction_length`.\n",
      "\n",
      "                See the demo notebook and code snippets for details.\n",
      "\n",
      "                Optionally, during training any missing values need to be replaced with zeros and indicated via the\n",
      "                `future_observed_mask`.\n",
      "\n",
      "                For multivariate time series, the `input_size` > 1 dimension is required and corresponds to the number of\n",
      "                variates in the time series per time step.\n",
      "            future_time_features (`torch.FloatTensor` of shape `(batch_size, prediction_length, num_features)`):\n",
      "                Required time features for the prediction window, which the model internally will add to `future_values`.\n",
      "                These could be things like \"month of year\", \"day of the month\", etc. encoded as vectors (for instance as\n",
      "                Fourier features). These could also be so-called \"age\" features, which basically help the model know \"at\n",
      "                which point in life\" a time-series is. Age features have small values for distant past time steps and\n",
      "                increase monotonically the more we approach the current time step. Holiday features are also a good example\n",
      "                of time features.\n",
      "\n",
      "                These features serve as the \"positional encodings\" of the inputs. So contrary to a model like BERT, where\n",
      "                the position encodings are learned from scratch internally as parameters of the model, the Time Series\n",
      "                Transformer requires to provide additional time features. The Time Series Transformer only learns\n",
      "                additional embeddings for `static_categorical_features`.\n",
      "\n",
      "                Additional dynamic real covariates can be concatenated to this tensor, with the caveat that these features\n",
      "                must but known at prediction time.\n",
      "\n",
      "                The `num_features` here is equal to `config.`num_time_features` + `config.num_dynamic_real_features`.\n",
      "            future_observed_mask (`torch.BoolTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, input_size)`, *optional*):\n",
      "                Boolean mask to indicate which `future_values` were observed and which were missing. Mask values selected\n",
      "                in `[0, 1]`:\n",
      "\n",
      "                - 1 for values that are **observed**,\n",
      "                - 0 for values that are **missing** (i.e. NaNs that were replaced by zeros).\n",
      "\n",
      "                This mask is used to filter out missing values for the final loss calculation.\n",
      "            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "                Mask to avoid performing attention on certain token indices. Mask values selected in `[0, 1]`:\n",
      "\n",
      "                - 1 for tokens that are **not masked**,\n",
      "                - 0 for tokens that are **masked**.\n",
      "\n",
      "                [What are attention masks?](../glossary#attention-mask)\n",
      "            decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n",
      "                Mask to avoid performing attention on certain token indices. By default, a causal mask will be used, to\n",
      "                make sure the model can only look at previous inputs in order to predict the future.\n",
      "            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n",
      "                Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:\n",
      "\n",
      "                - 1 indicates the head is **not masked**,\n",
      "                - 0 indicates the head is **masked**.\n",
      "\n",
      "            decoder_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
      "                Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:\n",
      "\n",
      "                - 1 indicates the head is **not masked**,\n",
      "                - 0 indicates the head is **masked**.\n",
      "\n",
      "            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
      "                Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n",
      "\n",
      "                - 1 indicates the head is **not masked**,\n",
      "                - 0 indicates the head is **masked**.\n",
      "\n",
      "            encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n",
      "                Tuple consists of `last_hidden_state`, `hidden_states` (*optional*) and `attentions` (*optional*)\n",
      "                `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)` (*optional*) is a sequence of\n",
      "                hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n",
      "            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
      "                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "                `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n",
      "                `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n",
      "\n",
      "                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n",
      "                blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
      "\n",
      "                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
      "                don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
      "                `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
      "            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
      "                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n",
      "                is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n",
      "                model's internal embedding lookup matrix.\n",
      "            use_cache (`bool`, *optional*):\n",
      "                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
      "                `past_key_values`).\n",
      "            output_attentions (`bool`, *optional*):\n",
      "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
      "                tensors for more detail.\n",
      "            output_hidden_states (`bool`, *optional*):\n",
      "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
      "                more detail.\n",
      "            return_dict (`bool`, *optional*):\n",
      "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      "\n",
      "\n",
      "        Returns:\n",
      "            [`transformers.modeling_outputs.Seq2SeqTSModelOutput`] or `tuple(torch.FloatTensor)`: A [`transformers.modeling_outputs.Seq2SeqTSModelOutput`] or a tuple of\n",
      "            `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various\n",
      "            elements depending on the configuration ([`TimeSeriesTransformerConfig`]) and inputs.\n",
      "\n",
      "        - **last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`) -- Sequence of hidden-states at the output of the last layer of the decoder of the model.\n",
      "\n",
      "          If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n",
      "          hidden_size)` is output.\n",
      "        - **past_key_values** (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`) -- Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "          `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n",
      "          `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n",
      "\n",
      "          Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n",
      "          blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
      "        - **decoder_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
      "          one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
      "\n",
      "          Hidden-states of the decoder at the output of each layer plus the optional initial embedding outputs.\n",
      "        - **decoder_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
      "          sequence_length)`.\n",
      "\n",
      "          Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n",
      "          self-attention heads.\n",
      "        - **cross_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
      "          sequence_length)`.\n",
      "\n",
      "          Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n",
      "          weighted average in the cross-attention heads.\n",
      "        - **encoder_last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) -- Sequence of hidden-states at the output of the last layer of the encoder of the model.\n",
      "        - **encoder_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
      "          one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
      "\n",
      "          Hidden-states of the encoder at the output of each layer plus the optional initial embedding outputs.\n",
      "        - **encoder_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
      "          sequence_length)`.\n",
      "\n",
      "          Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n",
      "          self-attention heads.\n",
      "        - **loc** (`torch.FloatTensor` of shape `(batch_size,)` or `(batch_size, input_size)`, *optional*) -- Shift values of each time series' context window which is used to give the model inputs of the same\n",
      "          magnitude and then used to shift back to the original magnitude.\n",
      "        - **scale** (`torch.FloatTensor` of shape `(batch_size,)` or `(batch_size, input_size)`, *optional*) -- Scaling values of each time series' context window which is used to give the model inputs of the same\n",
      "          magnitude and then used to rescale back to the original magnitude.\n",
      "        - **static_features** (`torch.FloatTensor` of shape `(batch_size, feature size)`, *optional*) -- Static features of each time series' in a batch which are copied to the covariates at inference time.\n",
      "\n",
      "\n",
      "    Examples:\n",
      "\n",
      "    ```python\n",
      "    >>> from huggingface_hub import hf_hub_download\n",
      "    >>> import torch\n",
      "    >>> from transformers import TimeSeriesTransformerForPrediction\n",
      "\n",
      "    >>> file = hf_hub_download(\n",
      "    ...     repo_id=\"hf-internal-testing/tourism-monthly-batch\", filename=\"train-batch.pt\", repo_type=\"dataset\"\n",
      "    ... )\n",
      "    >>> batch = torch.load(file)\n",
      "\n",
      "    >>> model = TimeSeriesTransformerForPrediction.from_pretrained(\n",
      "    ...     \"huggingface/time-series-transformer-tourism-monthly\"\n",
      "    ... )\n",
      "\n",
      "    >>> # during training, one provides both past and future values\n",
      "    >>> # as well as possible additional features\n",
      "    >>> outputs = model(\n",
      "    ...     past_values=batch[\"past_values\"],\n",
      "    ...     past_time_features=batch[\"past_time_features\"],\n",
      "    ...     past_observed_mask=batch[\"past_observed_mask\"],\n",
      "    ...     static_categorical_features=batch[\"static_categorical_features\"],\n",
      "    ...     static_real_features=batch[\"static_real_features\"],\n",
      "    ...     future_values=batch[\"future_values\"],\n",
      "    ...     future_time_features=batch[\"future_time_features\"],\n",
      "    ... )\n",
      "\n",
      "    >>> loss = outputs.loss\n",
      "    >>> loss.backward()\n",
      "\n",
      "    >>> # during inference, one only provides past values\n",
      "    >>> # as well as possible additional features\n",
      "    >>> # the model autoregressively generates future values\n",
      "    >>> outputs = model.generate(\n",
      "    ...     past_values=batch[\"past_values\"],\n",
      "    ...     past_time_features=batch[\"past_time_features\"],\n",
      "    ...     past_observed_mask=batch[\"past_observed_mask\"],\n",
      "    ...     static_categorical_features=batch[\"static_categorical_features\"],\n",
      "    ...     static_real_features=batch[\"static_real_features\"],\n",
      "    ...     future_time_features=batch[\"future_time_features\"],\n",
      "    ... )\n",
      "\n",
      "    >>> mean_prediction = outputs.sequences.mean(dim=1)\n",
      "    ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_model: 32\n",
      "   The [`TimeSeriesTransformerForPrediction`] forward method, overrides the `__call__` special method.\n",
      "\n",
      "    <Tip>\n",
      "\n",
      "    Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\n",
      "    instance afterwards instead of this since the former takes care of running the pre and post processing steps while\n",
      "    the latter silently ignores them.\n",
      "\n",
      "    </Tip>\n",
      "\n",
      "    Args:\n",
      "        past_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, input_size)`):\n",
      "            Past values of the time series, that serve as context in order to predict the future. The sequence size of\n",
      "            this tensor must be larger than the `context_length` of the model, since the model will use the larger size\n",
      "            to construct lag features, i.e. additional values from the past which are added in order to serve as \"extra\n",
      "            context\".\n",
      "\n",
      "            The `sequence_length` here is equal to `config.context_length` + `max(config.lags_sequence)`, which if no\n",
      "            `lags_sequence` is configured, is equal to `config.context_length` + 7 (as by default, the largest\n",
      "            look-back index in `config.lags_sequence` is 7). The property `_past_length` returns the actual length of\n",
      "            the past.\n",
      "\n",
      "            The `past_values` is what the Transformer encoder gets as input (with optional additional features, such as\n",
      "            `static_categorical_features`, `static_real_features`, `past_time_features` and lags).\n",
      "\n",
      "            Optionally, missing values need to be replaced with zeros and indicated via the `past_observed_mask`.\n",
      "\n",
      "            For multivariate time series, the `input_size` > 1 dimension is required and corresponds to the number of\n",
      "            variates in the time series per time step.\n",
      "        past_time_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_features)`):\n",
      "            Required time features, which the model internally will add to `past_values`. These could be things like\n",
      "            \"month of year\", \"day of the month\", etc. encoded as vectors (for instance as Fourier features). These\n",
      "            could also be so-called \"age\" features, which basically help the model know \"at which point in life\" a\n",
      "            time-series is. Age features have small values for distant past time steps and increase monotonically the\n",
      "            more we approach the current time step. Holiday features are also a good example of time features.\n",
      "\n",
      "            These features serve as the \"positional encodings\" of the inputs. So contrary to a model like BERT, where\n",
      "            the position encodings are learned from scratch internally as parameters of the model, the Time Series\n",
      "            Transformer requires to provide additional time features. The Time Series Transformer only learns\n",
      "            additional embeddings for `static_categorical_features`.\n",
      "\n",
      "            Additional dynamic real covariates can be concatenated to this tensor, with the caveat that these features\n",
      "            must but known at prediction time.\n",
      "\n",
      "            The `num_features` here is equal to `config.`num_time_features` + `config.num_dynamic_real_features`.\n",
      "        past_observed_mask (`torch.BoolTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, input_size)`, *optional*):\n",
      "            Boolean mask to indicate which `past_values` were observed and which were missing. Mask values selected in\n",
      "            `[0, 1]`:\n",
      "\n",
      "            - 1 for values that are **observed**,\n",
      "            - 0 for values that are **missing** (i.e. NaNs that were replaced by zeros).\n",
      "\n",
      "        static_categorical_features (`torch.LongTensor` of shape `(batch_size, number of static categorical features)`, *optional*):\n",
      "            Optional static categorical features for which the model will learn an embedding, which it will add to the\n",
      "            values of the time series.\n",
      "\n",
      "            Static categorical features are features which have the same value for all time steps (static over time).\n",
      "\n",
      "            A typical example of a static categorical feature is a time series ID.\n",
      "        static_real_features (`torch.FloatTensor` of shape `(batch_size, number of static real features)`, *optional*):\n",
      "            Optional static real features which the model will add to the values of the time series.\n",
      "\n",
      "            Static real features are features which have the same value for all time steps (static over time).\n",
      "\n",
      "            A typical example of a static real feature is promotion information.\n",
      "        future_values (`torch.FloatTensor` of shape `(batch_size, prediction_length)` or `(batch_size, prediction_length, input_size)`, *optional*):\n",
      "            Future values of the time series, that serve as labels for the model. The `future_values` is what the\n",
      "            Transformer needs during training to learn to output, given the `past_values`.\n",
      "\n",
      "            The sequence length here is equal to `prediction_length`.\n",
      "\n",
      "            See the demo notebook and code snippets for details.\n",
      "\n",
      "            Optionally, during training any missing values need to be replaced with zeros and indicated via the\n",
      "            `future_observed_mask`.\n",
      "\n",
      "            For multivariate time series, the `input_size` > 1 dimension is required and corresponds to the number of\n",
      "            variates in the time series per time step.\n",
      "        future_time_features (`torch.FloatTensor` of shape `(batch_size, prediction_length, num_features)`):\n",
      "            Required time features for the prediction window, which the model internally will add to `future_values`.\n",
      "            These could be things like \"month of year\", \"day of the month\", etc. encoded as vectors (for instance as\n",
      "            Fourier features). These could also be so-called \"age\" features, which basically help the model know \"at\n",
      "            which point in life\" a time-series is. Age features have small values for distant past time steps and\n",
      "            increase monotonically the more we approach the current time step. Holiday features are also a good example\n",
      "            of time features.\n",
      "\n",
      "            These features serve as the \"positional encodings\" of the inputs. So contrary to a model like BERT, where\n",
      "            the position encodings are learned from scratch internally as parameters of the model, the Time Series\n",
      "            Transformer requires to provide additional time features. The Time Series Transformer only learns\n",
      "            additional embeddings for `static_categorical_features`.\n",
      "\n",
      "            Additional dynamic real covariates can be concatenated to this tensor, with the caveat that these features\n",
      "            must but known at prediction time.\n",
      "\n",
      "            The `num_features` here is equal to `config.`num_time_features` + `config.num_dynamic_real_features`.\n",
      "        future_observed_mask (`torch.BoolTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, input_size)`, *optional*):\n",
      "            Boolean mask to indicate which `future_values` were observed and which were missing. Mask values selected\n",
      "            in `[0, 1]`:\n",
      "\n",
      "            - 1 for values that are **observed**,\n",
      "            - 0 for values that are **missing** (i.e. NaNs that were replaced by zeros).\n",
      "\n",
      "            This mask is used to filter out missing values for the final loss calculation.\n",
      "        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            Mask to avoid performing attention on certain token indices. Mask values selected in `[0, 1]`:\n",
      "\n",
      "            - 1 for tokens that are **not masked**,\n",
      "            - 0 for tokens that are **masked**.\n",
      "\n",
      "            [What are attention masks?](../glossary#attention-mask)\n",
      "        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n",
      "            Mask to avoid performing attention on certain token indices. By default, a causal mask will be used, to\n",
      "            make sure the model can only look at previous inputs in order to predict the future.\n",
      "        head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n",
      "            Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:\n",
      "\n",
      "            - 1 indicates the head is **not masked**,\n",
      "            - 0 indicates the head is **masked**.\n",
      "\n",
      "        decoder_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
      "            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:\n",
      "\n",
      "            - 1 indicates the head is **not masked**,\n",
      "            - 0 indicates the head is **masked**.\n",
      "\n",
      "        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
      "            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n",
      "\n",
      "            - 1 indicates the head is **not masked**,\n",
      "            - 0 indicates the head is **masked**.\n",
      "\n",
      "        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n",
      "            Tuple consists of `last_hidden_state`, `hidden_states` (*optional*) and `attentions` (*optional*)\n",
      "            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)` (*optional*) is a sequence of\n",
      "            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n",
      "        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
      "            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n",
      "            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n",
      "\n",
      "            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n",
      "            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
      "\n",
      "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
      "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
      "            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
      "        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
      "            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n",
      "            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n",
      "            model's internal embedding lookup matrix.\n",
      "        use_cache (`bool`, *optional*):\n",
      "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
      "            `past_key_values`).\n",
      "        output_attentions (`bool`, *optional*):\n",
      "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
      "            tensors for more detail.\n",
      "        output_hidden_states (`bool`, *optional*):\n",
      "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
      "            more detail.\n",
      "        return_dict (`bool`, *optional*):\n",
      "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      "\n",
      "\n",
      "    Returns:\n",
      "        [`transformers.modeling_outputs.Seq2SeqTSModelOutput`] or `tuple(torch.FloatTensor)`: A [`transformers.modeling_outputs.Seq2SeqTSModelOutput`] or a tuple of\n",
      "        `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various\n",
      "        elements depending on the configuration ([`TimeSeriesTransformerConfig`]) and inputs.\n",
      "\n",
      "    - **last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`) -- Sequence of hidden-states at the output of the last layer of the decoder of the model.\n",
      "\n",
      "      If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n",
      "      hidden_size)` is output.\n",
      "    - **past_key_values** (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`) -- Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "      `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n",
      "      `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n",
      "\n",
      "      Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n",
      "      blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
      "    - **decoder_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
      "      one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
      "\n",
      "      Hidden-states of the decoder at the output of each layer plus the optional initial embedding outputs.\n",
      "    - **decoder_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
      "      sequence_length)`.\n",
      "\n",
      "      Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n",
      "      self-attention heads.\n",
      "    - **cross_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
      "      sequence_length)`.\n",
      "\n",
      "      Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n",
      "      weighted average in the cross-attention heads.\n",
      "    - **encoder_last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) -- Sequence of hidden-states at the output of the last layer of the encoder of the model.\n",
      "    - **encoder_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
      "      one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
      "\n",
      "      Hidden-states of the encoder at the output of each layer plus the optional initial embedding outputs.\n",
      "    - **encoder_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
      "      sequence_length)`.\n",
      "\n",
      "      Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n",
      "      self-attention heads.\n",
      "    - **loc** (`torch.FloatTensor` of shape `(batch_size,)` or `(batch_size, input_size)`, *optional*) -- Shift values of each time series' context window which is used to give the model inputs of the same\n",
      "      magnitude and then used to shift back to the original magnitude.\n",
      "    - **scale** (`torch.FloatTensor` of shape `(batch_size,)` or `(batch_size, input_size)`, *optional*) -- Scaling values of each time series' context window which is used to give the model inputs of the same\n",
      "      magnitude and then used to rescale back to the original magnitude.\n",
      "    - **static_features** (`torch.FloatTensor` of shape `(batch_size, feature size)`, *optional*) -- Static features of each time series' in a batch which are copied to the covariates at inference time.\n",
      "\n",
      "\n",
      "Examples:\n",
      "\n",
      "```python\n",
      ">>> from huggingface_hub import hf_hub_download\n",
      ">>> import torch\n",
      ">>> from transformers import TimeSeriesTransformerForPrediction\n",
      "\n",
      ">>> file = hf_hub_download(\n",
      "...     repo_id=\"hf-internal-testing/tourism-monthly-batch\", filename=\"train-batch.pt\", repo_type=\"dataset\"\n",
      "... )\n",
      ">>> batch = torch.load(file)\n",
      "\n",
      ">>> model = TimeSeriesTransformerForPrediction.from_pretrained(\n",
      "...     \"huggingface/time-series-transformer-tourism-monthly\"\n",
      "... )\n",
      "\n",
      ">>> # during training, one provides both past and future values\n",
      ">>> # as well as possible additional features\n",
      ">>> outputs = model(\n",
      "...     past_values=batch[\"past_values\"],\n",
      "...     past_time_features=batch[\"past_time_features\"],\n",
      "...     past_observed_mask=batch[\"past_observed_mask\"],\n",
      "...     static_categorical_features=batch[\"static_categorical_features\"],\n",
      "...     static_real_features=batch[\"static_real_features\"],\n",
      "...     future_values=batch[\"future_values\"],\n",
      "...     future_time_features=batch[\"future_time_features\"],\n",
      "... )\n",
      "\n",
      ">>> loss = outputs.loss\n",
      ">>> loss.backward()\n",
      "\n",
      ">>> # during inference, one only provides past values\n",
      ">>> # as well as possible additional features\n",
      ">>> # the model autoregressively generates future values\n",
      ">>> outputs = model.generate(\n",
      "...     past_values=batch[\"past_values\"],\n",
      "...     past_time_features=batch[\"past_time_features\"],\n",
      "...     past_observed_mask=batch[\"past_observed_mask\"],\n",
      "...     static_categorical_features=batch[\"static_categorical_features\"],\n",
      "...     static_real_features=batch[\"static_real_features\"],\n",
      "...     future_time_features=batch[\"future_time_features\"],\n",
      "... )\n",
      "\n",
      ">>> mean_prediction = outputs.sequences.mean(dim=1)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(\"d_model:\", config.d_model) \n",
    "print(model.forward.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.948689</td>\n",
       "      <td>0.023647</td>\n",
       "      <td>0.373192</td>\n",
       "      <td>0.821342</td>\n",
       "      <td>0.633220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.661424</td>\n",
       "      <td>0.825077</td>\n",
       "      <td>0.724590</td>\n",
       "      <td>0.450877</td>\n",
       "      <td>0.162731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.101875</td>\n",
       "      <td>0.451638</td>\n",
       "      <td>0.786120</td>\n",
       "      <td>0.745441</td>\n",
       "      <td>0.410758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.254375</td>\n",
       "      <td>0.571630</td>\n",
       "      <td>0.900094</td>\n",
       "      <td>0.696035</td>\n",
       "      <td>0.417817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.809517</td>\n",
       "      <td>0.327158</td>\n",
       "      <td>0.782221</td>\n",
       "      <td>0.019583</td>\n",
       "      <td>0.990781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_0  feature_1  feature_2  feature_3  feature_4\n",
       "0   0.948689   0.023647   0.373192   0.821342   0.633220\n",
       "1   0.661424   0.825077   0.724590   0.450877   0.162731\n",
       "2   0.101875   0.451638   0.786120   0.745441   0.410758\n",
       "3   0.254375   0.571630   0.900094   0.696035   0.417817\n",
       "4   0.809517   0.327158   0.782221   0.019583   0.990781"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_steps = 100 \n",
    "features = 5 \n",
    "\n",
    "# Генерация случайных данных для временного ряда\n",
    "data = np.random.rand(time_steps, features)  \n",
    "\n",
    "df = pd.DataFrame(data, columns=[f\"feature_{i}\" for i in range(features)])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 100  # Количество временных шагов\n",
    "expected_features = 48  # Ожидаемое количество признаков\n",
    "inputs = torch.randn(1, time_steps, expected_features)  # (batch_size, time_steps, expected_features)\n",
    "past_time_features = torch.randn(1, time_steps, expected_features)  # Временные признаки\n",
    "past_observed_mask = torch.ones(1, time_steps) # Маска наблюдаемых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.FloatTensor{[1, 1, 2, 48]}, size=[-1, 0, -1]): the number of sizes provided (3) must be greater or equal to the number of dimensions in the tensor (4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Переводим модель в режим инференса\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# Отключаем вычисление градиентов\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpast_time_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_time_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpast_observed_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_observed_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# В outputs будет содержаться результат, например, прогноз\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs)\n",
      "File \u001b[1;32mc:\\Users\\2814\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\2814\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\2814\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\time_series_transformer\\modeling_time_series_transformer.py:1557\u001b[0m, in \u001b[0;36mTimeSeriesTransformerForPrediction.forward\u001b[1;34m(self, past_values, past_time_features, past_observed_mask, static_categorical_features, static_real_features, future_values, future_time_features, future_observed_mask, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, output_hidden_states, output_attentions, use_cache, return_dict)\u001b[0m\n\u001b[0;32m   1554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m future_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1555\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1557\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_time_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_time_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_observed_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_observed_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_categorical_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatic_categorical_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_real_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatic_real_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfuture_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuture_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfuture_time_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuture_time_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1572\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1573\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1575\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1577\u001b[0m prediction_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1578\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\2814\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\2814\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\2814\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\time_series_transformer\\modeling_time_series_transformer.py:1378\u001b[0m, in \u001b[0;36mTimeSeriesTransformerModel.forward\u001b[1;34m(self, past_values, past_time_features, past_observed_mask, static_categorical_features, static_real_features, future_values, future_time_features, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, output_hidden_states, output_attentions, use_cache, return_dict)\u001b[0m\n\u001b[0;32m   1375\u001b[0m use_cache \u001b[38;5;241m=\u001b[39m use_cache \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[0;32m   1376\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1378\u001b[0m transformer_inputs, loc, scale, static_feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_network_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_time_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_time_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_observed_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_observed_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_categorical_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatic_categorical_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_real_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatic_real_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfuture_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuture_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfuture_time_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuture_time_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1386\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1389\u001b[0m     enc_input \u001b[38;5;241m=\u001b[39m transformer_inputs[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mcontext_length, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\2814\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\time_series_transformer\\modeling_time_series_transformer.py:1287\u001b[0m, in \u001b[0;36mTimeSeriesTransformerModel.create_network_inputs\u001b[1;34m(self, past_values, past_time_features, static_categorical_features, static_real_features, past_observed_mask, future_values, future_time_features)\u001b[0m\n\u001b[0;32m   1285\u001b[0m     embedded_cat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedder(static_categorical_features)\n\u001b[0;32m   1286\u001b[0m     static_feat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((embedded_cat, static_feat), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m-> 1287\u001b[0m expanded_static_feat \u001b[38;5;241m=\u001b[39m \u001b[43mstatic_feat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_feat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1289\u001b[0m \u001b[38;5;66;03m# all features\u001b[39;00m\n\u001b[0;32m   1290\u001b[0m features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((expanded_static_feat, time_feat), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expand(torch.FloatTensor{[1, 1, 2, 48]}, size=[-1, 0, -1]): the number of sizes provided (3) must be greater or equal to the number of dimensions in the tensor (4)"
     ]
    }
   ],
   "source": [
    "model.eval()  # Переводим модель в режим инференса\n",
    "\n",
    "with torch.no_grad():  # Отключаем вычисление градиентов\n",
    "        outputs = model(inputs, \n",
    "                     past_time_features=past_time_features, \n",
    "                     past_observed_mask=past_observed_mask)\n",
    "# В outputs будет содержаться результат, например, прогноз\n",
    "print(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
